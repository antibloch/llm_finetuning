
MODEL_NAME: "meta-llama/Meta-Llama-3-8B-Instruct" # Huggingface or unsloth model name
DATASET_NAME: "commonsense_qa" # Dataset name from Huggingface datasets
max_seq_length: 2048 # Maximum sequence length for the model
dtype: null # Autodetects for your GPU
load_in_4bit: true # Use 4-bit quantization

# LoRA specific parameters
lora_alpha: 16
lora_r: 16
lora_dropout: 0
bias: "none"
use_gradient_checkpointing: "unsloth"
random_state: 3407

# Training parameters
learning_rate: 2.0e-5 # Represented as a float
per_device_batch_size: 2
gradient_accumulation_steps: 4
max_steps: 60
output_dir: "outputs"
optim: "adamw_8bit"