MODEL_NAME: "meta-llama/Meta-Llama-3-8B-Instruct" # Huggingface or unsloth model name
DATASET_NAME: "commonsense_qa" # Dataset name from Huggingface datasets
max_seq_length: 2048 # Maximum sequence length for the model
dtype: null # Autodetects for your GPU
load_in_4bit: true # Use 4-bit quantization
load_in_8bit: false # Use 8-bit quantization

# LoRA specific parameters
lora_alpha: 16
lora_r: 16
lora_dropout: 0
bias: "none"
use_gradient_checkpointing: "unsloth"
random_state: 3407

# Training parameters
learning_rate: 2.0e-5 # Represented as a float
per_device_batch_size: 2
gradient_accumulation_steps: 4
max_steps: 60
output_dir: "outputs"
optim: "adamw_8bit" # "adamw_hf" or "adamw_8bit" or "adamw_torch" or "paged_adamw_32bit"
num_train_epochs: 1
weight_decay: 0.01
max_grad_norm: 1.0
max_length: 1024
lr_scheduler_type: "cosine"
warmup_ratio: 0.1
logging_steps: 10
save_steps: 200
save_total_limit: 2
dataloader_num_workers: 2